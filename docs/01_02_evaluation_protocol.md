Month 1–2: Instruction Following (The Stress Test)
What I’m looking for here: The goal for these first two months is to see if the model can actually keep its head under pressure. I’m putting its structural integrity to the test to see if it can stick to the brief when the instructions get complicated.

Where I’m focusing my review:
The Rule Book (Constraint Hierarchy): I’m checking if the model knows the difference between a "hard rule" and a "suggestion." If I tell it "Do not use the word 'red'," that is non-negotiable. I want to see if it prioritizes these constraints or if it gets lazy.

Breaking the Format (Stability): Can the model keep a JSON or Markdown table clean, even during a long conversation? I’m looking for "hallucinated" brackets or broken tables that would crash an app.

Staying in Character (Persona Leakage): If I've given the model a specific persona, I want to see if it slips up. I’ll be trying to "tempt" it to break character to see how resilient it is.

The Face-Off:
Control Model: GPT-4o (The benchmark for reliability).

Challenger Model: Llama 3.1 (To see if open-source can keep up).

My Perspective as an Evaluator:
I'm not just looking for "correct" answers. I'm looking for consistency. I’ll be flagging every time a model drops the ball on a constraint, messes up a format, or forgets who it’s supposed to be. If the Challenger model can't stay on track as well as GPT-4o, I’ll be documenting exactly where it cracks.