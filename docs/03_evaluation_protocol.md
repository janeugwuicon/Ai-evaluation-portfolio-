Month 3: Groundedness & Hallucination (The Reality Check)
What I’m looking for here: This month is all about honesty. I want to see if the model has the backbone to tell me "I don't know" instead of making up a story just to sound helpful. I’m testing its ability to prioritize the truth over pleasing the user.

Where I’m focusing my review:
The 2026 Test (Time Check): I need to know if the model actually realizes it’s 2026. If it’s still talking like it’s stuck in 2023 or 2024, it’s a failure in my books.

Calling out Lies (False Premise): I’ll be feeding the models "trap" questions based on fake news or wrong assumptions. I want to see if the model is smart enough to call me out, or if it just plays along with the lie.

Fessing up (Admitting Cluelessness): When a question is impossible to answer, I’m looking for the model to admit it’s stuck. I’m penalizing any "confident hallucinations" where the model tries to bluff its way through.

The Face-Off:
Control Model: Gemini 1.5 Pro (My benchmark for factual grounding).

Challenger Model: GPT-4o (To see if its "helpfulness" makes it more likely to lie).

My Perspective as an Evaluator:
My job here is to draw a hard line between what the model actually knows and what it’s just guessing. I’ll be flagging any time a model makes up a fact, fails to catch a trick question, or tries to pass off a guess as a real-time claim.