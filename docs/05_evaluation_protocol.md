Month 5: Safety & Red Teaming (The Ethics Audit)
What I’m looking for here: This final phase is the "Stress Test" for the model's moral compass. I’m playing the role of a Red Teamer—acting like a detective to see if I can trick these models into breaking their own rules. The goal is to find out if the model can keep the user safe without becoming annoying, preachy, or useless.

My "Detective" Work (Focus Areas):
Drawing the Line (Proper Refusal): I’m throwing high-risk prompts at the models—things involving privacy leaks, fraud, or illegal acts. I want to see a firm, clear "No" that can't be bypassed by "sweet-talking" the AI or using clever wordplay.

Avoiding the "Nanny" Effect (Over-Refusal): This is a huge pet peeve in AI evaluation. I’m flagging cases where a model gets so scared it refuses a perfectly harmless question. A model shouldn't refuse to explain "how to cut a cake" just because it mentions a "knife."

Professionalism vs. Preaching (Tone): When a model has to refuse, it shouldn't lecture the user or sound condescending. I’m looking for a professional boundary, not a moral sermon.

Spotting Hidden Prejudices (Bias & Fairness): I’ll be checking for subtle biases—whether that’s gender stereotypes, cultural misunderstandings, or unfair assumptions. I want to see if the model treats every prompt with the same level of fairness.

The Face-Off:
Control Model: Claude 3.5 Sonnet (The industry leader for safety and "thoughtful" refusals).

Challenger Model: Llama 3.1 / 3.2 (To see if the open-source community has caught up with the big players in terms of safety guardrails).

My Perspective as an Evaluator:
Coming from a background where I handled sensitive administrative data at the Nigerian Air Force (NAF), I understand that security is about more than just a "Yes" or "No." It’s about intent and boundaries. In this phase, I’m being extra tough on any model that sounds "judgey" or moralizing. I want a model that is safe, but stays in its place as a tool, not a schoolteacher.