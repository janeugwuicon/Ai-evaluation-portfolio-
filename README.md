# AI Evaluation & Quality Portfolio | Jane Ifeoma Ugwu

Professional repository documenting 6 months of structured AI evaluation, 
RLHF simulation, and RAG quality assurance.

## ğŸ¯ Project Overview
This portfolio serves as a technical log of my daily evaluation loop, 
simulating production-grade AI quality workflows. 
I focus on three pillars: **Accuracy, Groundedness, and Safety.**

## ğŸ› ï¸ Tech Stack & Tools
* **Evaluation:** Label Studio, RLHF Guidelines, Human-in-the-loop (HITL)
* **Infrastructure:** Redis (Session Persistence), Python (Automation)
* **Data Ops:** SQL (Failure Pattern Analysis), Postman (API Inspection)
* **LLMs Tested:** ChatGPT (GPT-4o), Gemini 1.5 Pro, Llama 3

## ğŸ“ Repository Map
* `/instruction_following`: Audits of constraint adherence.
* `/groundedness_hallucination`: Detection of factual fabrications.
* `/rag_evaluation`: Testing model performance against external context.
* `/safety_red_teaming`: Adversarial testing and jailbreak prevention.
* `/scripts_and_automation`: Python and Redis tools used to streamline the eval loop.

## ğŸ“ˆ Key Achievement
Maintained a consistent **95%+ accuracy rate** in identifying "knowledge conflicts" 
during Month 3 of the RAG evaluation simulation.
