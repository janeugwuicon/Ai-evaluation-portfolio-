My AI Eval & Quality Journey (Building in Public)
This is my technical log where Iâ€™m documenting my 6-month deep dive into AI evaluation, RLHF, and RAG quality. Iâ€™m building this project bit by bit every day, so this repo is very much a "work in progress."

ğŸ¯ What Iâ€™m Chasing
Iâ€™m simulating what itâ€™s like to manage AI quality in a real production environment. No be just to tell the AI "how far," Iâ€™m looking at three main things: Is the answer correct? Is it based on the facts I gave it? And is it safe for people to use?

ğŸ› ï¸ The Tools Iâ€™m Using (As I go)
Since Iâ€™m building this daily, my stack is still evolving, but hereâ€™s what Iâ€™m currently rocking:

Eval Method: Using Label Studio and proper RLHF guidelines to grade the models. Itâ€™s a lot of "Human-in-the-loop" workâ€”eye-service is actually required here!

The Code Side: Python is my go-to for automating all the repetitive stuff. Iâ€™m still figuring out the best way to handle session persistence as the project grows.

The Models: Right now, Iâ€™m putting GPT-4o, Gemini 1.5 Pro, and Llama 3 through their paces.

ğŸ“ Repository Map (The Layout)
Iâ€™m populating these folders one-one as I move:

/instruction_following: Auditing if the AI can follow simple rules without "doing its own."

/groundedness_hallucination: Checking if the model is telling the truth or just vibes and stories.

/rag_evaluation: This is where I test how well the model reads the context I provide.

/safety_red_teaming: Trying my best to "break" the model to see if itâ€™ll say what itâ€™s not supposed to.

/scripts_and_automation: My Python scripts that make the whole evaluation loop run faster.
